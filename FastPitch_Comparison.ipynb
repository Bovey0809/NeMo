{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59b09bb",
   "metadata": {
    "id": "8d0bbac2"
   },
   "source": [
    "# Fastpitch comparison\n",
    "\n",
    "In this tutorial, we will generate wav files and spectrogram for the text from dev set using FastPitch trained on **LJSpeech**  and pretrained UnivNetModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3a10a",
   "metadata": {
    "id": "c3bdf1ed"
   },
   "source": [
    "## Synthesize Samples from Finetuned Checkpoints\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cddb9",
   "metadata": {},
   "source": [
    "To run this notebook you would need:\n",
    "\n",
    "    - New FastPitch.nemo file\n",
    "    - model_config.yaml configuration file to be used to load the old FastPitch.nemo file\n",
    "    - LJSpeech dataset\n",
    "    - nvidia_ljspeech_val_mini.json manifest file to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ef210",
   "metadata": {
    "id": "886c91dc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from nemo.collections.tts.models import HifiGanModel, UnivNetModel\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "UnivNetModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a093dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univnet\n",
    "vocoder = UnivNetModel.from_pretrained(\"tts_en_libritts_univnet\")\n",
    "vocoder = vocoder_u.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7875a1",
   "metadata": {
    "id": "0a4c986f"
   },
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker = None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Arguments:\n",
    "    spec_gen_model -- Instance of FastPitch model\n",
    "    vocoder_model -- Instance of a vocoder model (HiFiGAN in our case)\n",
    "    str_input -- Text input for the synthesis\n",
    "    speaker -- Speaker number (in the case of a multi-speaker model -- in the mixing case)\n",
    "    \n",
    "    Returns:\n",
    "    spectrogram, waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    parser_model = spec_gen_model\n",
    "    with torch.no_grad():\n",
    "        parsed = parser_model.parse(str_input)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker = speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d94a46",
   "metadata": {
    "id": "0153bd5a"
   },
   "source": [
    "Specify the speaker id, duration mins and mixing variable to find the relevant checkpoint from the exp_base_dir and compare the synthesized audio with validation samples of the new speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f93c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastPitchModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c3186",
   "metadata": {
    "id": "8901f88b"
   },
   "outputs": [],
   "source": [
    "# change this\n",
    "path_to_new_nemo_file = '/data/speech/LJSpeech/FastPitch/1xV100_BS8/checkpoints/FastPitch.nemo'\n",
    "\n",
    "spec_model_1 = FastPitchModel.from_pretrained('tts_en_fastpitch', override_config_path='/workspace/NeMo/model_config.yaml')\n",
    "spec_model_1.eval().cuda()\n",
    "spec_model_2 = FastPitchModel.restore_from(path_to_new_nemo_file)\n",
    "spec_model_2.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6afc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_path = '/data/speech/LJSpeech/LJSupplementary/nvidia_ljspeech_val_mini.json'\n",
    "val_records = list(json_reader(manifest_path))\n",
    "len(val_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb6457",
   "metadata": {},
   "source": [
    "# GENERATE INFERENCE EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, val_record in enumerate(val_records):\n",
    "    print (\"Real validation audio\")\n",
    "    _speaker = 1\n",
    "    ipd.display(ipd.Audio(val_record['audio_filepath'], rate=22050))\n",
    "    \n",
    "    print (\"SYNTHESIZED FOR Text: {}\".format(val_record['text']))\n",
    "    \n",
    "    print(\"Old Fastpitch ckpt\")\n",
    "    spec, audio = infer(spec_model_1, vocoder, val_record['text'], speaker = None)\n",
    "    ipd.display(ipd.Audio(audio, rate=22050))\n",
    "    print(\"New Fastpitch ckpt\")\n",
    "    spec, audio = infer(spec_model_2, vocoder, val_record['text'], speaker = None)\n",
    "    ipd.display(ipd.Audio(audio, rate=22050))\n",
    "    plt.show()\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39c662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
